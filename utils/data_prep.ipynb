{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "categories_df = pd.read_csv('../data/amazon_categories.csv')\n",
    "products_df = pd.read_csv('../data/amazon_products.csv')\n",
    "data = pd.merge(categories_df, products_df, left_on='id', right_on='category_id')\n",
    "data = data.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000\n",
    "print(data['category_id'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_to_keep_str = ['45','46','47','48','49','50','52','71','72','84','90','91','97','101','103','104','105','107','108','109','110','111','112','113','114','116','118','120','121','122','123','173','174', '270']\n",
    "category_id_to_keep = [int(id) for id in category_id_to_keep_str]\n",
    "filtered_data = data[data['category_id'].isin(category_id_to_keep)]\n",
    "filtered_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_data['boughtInLastMonth'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data[~filtered_data['boughtInLastMonth'].isin([0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "container_name = \"amazondata1\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def download_image(url):\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BytesIO(response.content)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to download image from {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def upload_to_blob(image_data, blob_name):\n",
    "    try:\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        blob_client.upload_blob(image_data, overwrite=True)\n",
    "        return blob_client.url\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {blob_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_row(row):\n",
    "    url = row['imgUrl']\n",
    "    image_data = download_image(url)\n",
    "    if image_data:\n",
    "        blob_name = os.path.basename(urlparse(url).path)\n",
    "        blob_url = upload_to_blob(image_data, blob_name)\n",
    "        return row.name, blob_name, blob_url\n",
    "    return row.name, None, None\n",
    "\n",
    "def process_batch(batch):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_row, row) for _, row in batch.iterrows()]\n",
    "        results = [future.result() for future in as_completed(futures)]\n",
    "    return results\n",
    "\n",
    "batch_size = 1000\n",
    "total_batches = len(filtered_data) // batch_size + (1 if len(filtered_data) % batch_size else 0)\n",
    "\n",
    "filtered_data['blob_name'] = None\n",
    "filtered_data['blob_url'] = None\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(filtered_data), batch_size):\n",
    "    batch = filtered_data.iloc[i:i+batch_size]\n",
    "    results = process_batch(batch)\n",
    "    \n",
    "    for index, blob_name, blob_url in results:\n",
    "        filtered_data.loc[index, 'blob_name'] = blob_name\n",
    "        filtered_data.loc[index, 'blob_url'] = blob_url\n",
    "    \n",
    "    batch_number = i // batch_size + 1\n",
    "    elapsed_time = time.time() - start_time\n",
    "    avg_time_per_batch = elapsed_time / batch_number\n",
    "    estimated_time_remaining = avg_time_per_batch * (total_batches - batch_number)\n",
    "    \n",
    "    print(f\"Processed batch {batch_number}/{total_batches}\")\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Estimated time remaining: {estimated_time_remaining:.2f} seconds\")\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    time.sleep(1)  \n",
    "\n",
    "print(\"Processing complete!\")\n",
    "print(filtered_data)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_description(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the webpage: {url}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    # Extract title\n",
    "    title_element = soup.select_one(\"span#productTitle\")\n",
    "    title = title_element.text.strip() if title_element else None\n",
    "\n",
    "    # Extract rating\n",
    "    rating = None\n",
    "    rating_element = soup.select_one(\"span.a-icon-alt\")\n",
    "    if rating_element:\n",
    "        rating_text = rating_element.text\n",
    "        rating = rating_text.split()[0] if rating_text else None\n",
    "\n",
    "    # Extract description\n",
    "    description = None\n",
    "    script_tags = soup.find_all(\"script\", type=\"application/ld+json\")\n",
    "    for script in script_tags:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            if \"description\" in data:\n",
    "                description = data[\"description\"]\n",
    "                break\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"rating\": rating,\n",
    "        \"description\": description\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scrape_description('https://www.amazon.com/dp/B001O867N6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "df = filtered_data\n",
    "\n",
    "def scrape_description(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "    }\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            description = None\n",
    "            \n",
    "            prod_desc = soup.find('div', {'id': 'productDescription'})\n",
    "            if prod_desc:\n",
    "                description = prod_desc.text.strip()\n",
    "            \n",
    "            if not description:\n",
    "                feature_bullets = soup.find('div', {'id': 'feature-bullets'})\n",
    "                if feature_bullets:\n",
    "                    description = feature_bullets.text.strip()\n",
    "            \n",
    "            if not description:\n",
    "                tech_details = soup.find('div', {'id': 'technical-details'})\n",
    "                if tech_details:\n",
    "                    description = tech_details.text.strip()\n",
    "            \n",
    "            if not description:\n",
    "                prod_info = soup.find('div', {'id': 'prodDetails'})\n",
    "                if prod_info:\n",
    "                    description = prod_info.text.strip()\n",
    "            \n",
    "            if not description:\n",
    "                desc_classes = ['a-section a-spacing-medium a-spacing-top-small', 'a-expander-content a-expander-partial-collapse-content']\n",
    "                for class_name in desc_classes:\n",
    "                    desc_div = soup.find('div', {'class': class_name})\n",
    "                    if desc_div:\n",
    "                        description = desc_div.text.strip()\n",
    "                        break\n",
    "            \n",
    "            if not description:\n",
    "                keywords = ['description', 'about this item', 'product description', 'overview']\n",
    "                paragraphs = soup.find_all('p')\n",
    "                for p in paragraphs:\n",
    "                    if any(keyword in p.text.lower() for keyword in keywords):\n",
    "                        description = p.text.strip()\n",
    "                        break\n",
    "            \n",
    "            if description:\n",
    "                description = re.sub(r'\\s+', ' ', description)  \n",
    "                description = description.replace('\\n', ' ').strip() \n",
    "                return description\n",
    "            else:\n",
    "                return \"Description not found\"\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to scrape {url}: {str(e)}\")\n",
    "                return \"Error: Unable to scrape\"\n",
    "            else:\n",
    "                sleep(random.uniform(1, 3))  # Random delay between retries\n",
    "\n",
    "    return \"Error: Max retries reached\"  \n",
    "def process_batch(urls):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(scrape_description, url): url for url in urls}\n",
    "        results = {}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                results[url] = future.result()\n",
    "            except Exception as exc:\n",
    "                print(f'{url} generated an exception: {exc}')\n",
    "                results[url] = \"Error: Exception occurred\"\n",
    "    return results\n",
    "\n",
    "batch_size = 100\n",
    "num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "\n",
    "descriptions = {}\n",
    "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df))\n",
    "    batch_urls = df['productURL'].iloc[start_idx:end_idx].tolist()\n",
    "    batch_results = process_batch(batch_urls)\n",
    "    descriptions.update(batch_results)\n",
    "    sleep(random.uniform(1, 3)) \n",
    "\n",
    "df['description'] = df['productURL'].map(descriptions)\n",
    "\n",
    "#df.to_csv('updated_dataset.csv', index=False)\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/updated_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv('../data/filtered_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopping-copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
